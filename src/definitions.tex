% *** Th√©orie
% *** Exemples d'applications

\section{Methods}
\label{sec:definitions}

The HLDA is an extension of the multiclass LDA.
It's goal is to allow heteroscediscity: The classes respective Covariance matrix can be different.
First, we will explain the bases of the LDA.
Then we will analyze how the HLDA is an extension of these bases.

\subsection{Notations}

The following notations are used throughout the rest of the document:

\begin{itemize*}
  \item Let $X_i$, $i \in \llbracket 1 \ldots N \rrbracket$ be a $n$-dimensional vector corresponding to a sample.
  \item We consider $J$ classes and refer to a class by $C_j$, $j \in \llbracket 1 \ldots J \rrbracket$
  \item The number of samples in a class is given by $N_j$, $j \in \llbracket 1 \ldots J \rrbracket$
  \item The notation $X_i^{(j)}$, $j \in \llbracket 1 \ldots J \rrbracket$, $i \in \llbracket 1 \ldots N_j \rrbracket$
    represents the ith sample of the jth class
  \item For each classes, the mean $\mu_j$ and the covariance $\Sigma_j$ are defined as:
    $$\mu_j = \frac{1}{N_j}\sum\limits_{i = 1}^{N_j} X_i^{(j)}$$
    $$\Sigma_j = \frac{1}{N_j}\sum\limits_{i = 1}^{N_j} (X_i^{(j)} - \mu_j)(X_i^{(j)} - \mu_j)^t$$
  \item Finally the mean and covariance of the full dataset are defined as:
    $$\mu = \frac{1}{N}\sum\limits_{i = 1}^{N} X_i$$
    $$\Sigma = \frac{1}{N}\sum\limits_{i = 1}^{N_j} (X_i - \mu)(X_i - \mu)^t$$
\end{itemize*}

\subsection{Multiclass LDA}

\subsubsection{Purpose}

The main goal of the multiclass LDA is to find a predictor that will allow to find the class that best correspond to a sample X.
In order to do that, we need to find a separator and each class will be projected on it.
The main goal is to find the best separator, so for a sample X, it will be easy to find which class
best correspond to X.
In a second time, it is possible to perform a dimension reduction on the separator.

To summarize, the LDA has two objectives:
- Find a separator that will allow a sample to be classified.
- Perform a dimension reduction to classify faster.

Now we will explain how the multiclass LDA works. 

\subsubsection{Methodology}

We want to find the class that best correspond to a vector x.
In order to easily differentiate classes, we need:

\begin{enumerate}
\item Each projected sample of the same class should be very close: we will call this the within-class ratio $S_W$.
\item The distance between each class data should be very large: we will call this the between-class ratio $S_B$.
\end{enumerate}

We will try to find the smallest within-class ratio and the biggest between-class ratio in order to best isolate each class.

So the function that we want to maximize is:
$$S = \frac{w^t S_B w}{w^t S_W w}$$
with:
\begin{itemize*}
  \item $S$ the ratio to maximize
  \item $S_B$ the between-class ratio
  \item $S_W$ the within-class ratio
  \item $w$ the projection matrix
\end{itemize*}

Welling\cite{welling} demonstrates that the expressions of $S_B$ and $S_W$ for the multiclass LDA are:
$$S_B = \frac{1}{C} * \sum\limits_{i = 1}^C (\mu_i - \mu)(\mu_i - \mu)^t$$
$$S_W = \frac{1}{C} * \sum\limits_{i = 1}^C \sum\limits_{\forall x \in C_i} (x - \mu_i)(x - \mu_i)^t$$
With:
\begin{itemize*}
  \item $\mu$ the mean of the class means
  \item $\mu_i$ the mean of the ith class
\end{itemize*}

For a multiclass LDA, the best projection matrix for the ith dimension satisfy the equation:
$$(S_B - \lambda_i S_W)w_{i}^* = 0$$

Gutierrez-Osuna\cite{gutierrez.osuna} explains that the projections with the maximum class separability information are the eigenvectors corresponding to the largest eigenvalues of $S_{W}^{-1} S_B$.

