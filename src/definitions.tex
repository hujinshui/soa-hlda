% *** ThÃ©orie
% *** Exemples d'applications

\section{Definitions}
\label{sec:definitions}

The HLDA is an extension of the multiclass LDA.
It's goal is to allow heteroscediscity: The classes respective Covariance matrix
can be different.
First, we will explain the bases of the LDA.
Then we will analyze how the HLDA is an extension of these bases.

\subsection{Theory}

We suppose that we have a database in which are store vectors that are grouped in classes.
The main goal of the multiclass LDA is to find a predictor that will allow to find the class that best
correspond to a sample X.
In order to do that, we need to find a separator and each class will be projected on it.
The main goal is to find the best separator, so for a sample X, it will be easy to find which class
best correspond to X.
In a second time, it is possible to perform a dimension reduction on the separator.

To summurize, the LDA has two objectives:
- Find a separator that will allow a sample to be classified.
- Perform a dimension reduction to classify faster.

Now we will explain how the multiclass LDA works. 

\subsubsection{Multiclass LDA}

We suppose that we have a set of sample in EXPRESSION(n) dimension, labelized by their respective
class.
We suppose that the covariance matrix EXPRESSION(SIGMA) of all class is the same (homoscedasticity)
The mean of each class is notated EXPRESSION(MU).
The number of different classes is EXPRESSION(C).

We want to find the class that best correspond to a vector x.
In order to easily differenciate classes, we need:
- Each projected sample of the same class should be very close: we will call this the within-class ratio.
- The distance between each class data set projection should be very large: we will call this the between-class ratio.

We will try to find the smallest within-class ratio and the biggest between-class ratio in order to best
isolate each class.
So the function that we want to maximize is:
EXPRESSION( S = w' Sb w / w' Sw w)
with:
S the ration to maximize
Sb the between-class ratio
Sw the within-class ratio
w the a projection vector 

The expressions of Sb and Sw in the multiclass LDA case are:
Sb = (1 / C) * SUM(1 => C) (MUi - MU)(MUi - MU)'
Sw = (1 / C) * SUM(1 => C) SUM(x - MUi)(X - MUi)'
With:
MU the mean of the class means
and MUi the mean of the ith class

We then want to find W that maximize S.
We will call this special W, W*
Knowing that the best W* that we can choose in a two class separation is:
W* = Sw-1(MU1 - MU2)

We can extend this expression to the multiclass case:
(Sb - LiSw)Wi* = 0

This means that the projections with the maximum class separability information are
the eigenvectors corresponding to the largest eigenvalues of Sw-1Sb.

\subsubsection{HLDA}

The linear discriminant analysis is based on the fact that the Covariance Matrix of
each class is the same. This is called homoscedasticity.
The difference between the LDA and the HLDA is that the second one is not based on the
homoscedasticity.
The HLDA suppose that the covariance matrix of each class is different.
